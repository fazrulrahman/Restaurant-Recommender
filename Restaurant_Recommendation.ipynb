{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Restaurant - Recommendation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVBgxBCvvdohGaC+ekP6KD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fazrulrahman/Restaurant-Recommender/blob/master/Restaurant_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsoqP13bRhkX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1659ee14-8ecc-4de2-b45b-cc0c5d2de515"
      },
      "source": [
        "## loading required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Preprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "import re\n",
        "\n",
        "# Distance between vector in multi-dimentional space\n",
        "import os\n",
        "import gensim\n",
        "from gensim.models import LsiModel\n",
        "from gensim import models\n",
        "from gensim import corpora\n",
        "from gensim.utils import lemmatize\n",
        "import nltk\n",
        "from gensim.parsing.preprocessing import remove_stopwords, stem_text\n",
        "from gensim.parsing.preprocessing import strip_numeric\n",
        "import pandas as pd\n",
        "from gensim import similarities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (47.3.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.6/dist-packages (0.5.10)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEEtcxUsW267",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## User input\n",
        "\n",
        "# cuisine = input(\"What do you like to Eat(Cuisine Type):\")\n",
        "# budget = input(\"What's your bugdet for one person:\")\n",
        "# location = input(\"Preferred Location:\")\n",
        "# additional = input(\"Any other comments:\")\n",
        "\n",
        "cuisine = \"Chinese\"\n",
        "# budget = 500\n",
        "location = \"mg road\"\n",
        "additional = \"pork\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qITCAXPIXK-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Downloading dataset\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1j-vywORX8WjldF0OsTo_fwRmebf8fGfm',\n",
        "                                    dest_path='./data/RestoInfo - Recommendation.csv',\n",
        "                                    unzip=False)\n",
        "\n",
        "reviews = pd.read_csv(\"data/RestoInfo - Recommendation.csv\")\n",
        "reviews['key'] = reviews['name'] + reviews['listed_in(type)'] + reviews['listed_in(city)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-FLnEtd2qqT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# @title Funtions for Data Normalization and data cleaning\n",
        "## Funtions for Data Normalization\n",
        "\n",
        "# Restaurant Type\n",
        "def atomizing_rest_type():\n",
        "  temp_rest_type = reviews[['name','listed_in(type)','listed_in(city)','rest_type']]\n",
        "  temp_rest_type['rest_type'] = temp_rest_type['rest_type'].str.split(',').tolist()\n",
        "  temp_rest_type['rest_type'] = temp_rest_type['rest_type'].fillna('0')\n",
        "\n",
        "  temp_rest_type['key'] = temp_rest_type['name'] + temp_rest_type['listed_in(type)'] + temp_rest_type['listed_in(city)']\n",
        "  temp_rest_type1 = temp_rest_type[['key','rest_type']]\n",
        "\n",
        "  rest_type = pd.DataFrame([[x] + [z] for x, y in temp_rest_type1.values for z in y],columns=temp_rest_type1.columns)\n",
        "\n",
        "  rest_type = rest_type.merge(temp_rest_type[['key','name','listed_in(type)','listed_in(city)']],how = 'left',on = 'key')\n",
        "  rest_type = rest_type[['key','name','listed_in(type)','listed_in(city)','rest_type']]\n",
        "  return(rest_type)\n",
        "\n",
        "# Liked Dishes\n",
        "def atomizing_liked_dishes():\n",
        "  temp_dish_liked = reviews[['name','listed_in(type)','listed_in(city)','dish_liked']]\n",
        "  temp_dish_liked['dish_liked'] = temp_dish_liked['dish_liked'].str.split(',').tolist()\n",
        "  temp_dish_liked['dish_liked'] = temp_dish_liked['dish_liked'].fillna('0')\n",
        "\n",
        "  temp_dish_liked['key'] = temp_dish_liked['name'] + temp_dish_liked['listed_in(type)'] + temp_dish_liked['listed_in(city)']\n",
        "  temp_dish_liked1 = temp_dish_liked[['key','dish_liked']]\n",
        "\n",
        "  dish_liked = pd.DataFrame([[x] + [z] for x, y in temp_dish_liked1.values for z in y],columns=temp_dish_liked1.columns)\n",
        "\n",
        "  dish_liked = dish_liked.merge(temp_dish_liked[['key','name','listed_in(type)','listed_in(city)']],how = 'left',on = 'key')\n",
        "  dish_liked = dish_liked[['key','name','listed_in(type)','listed_in(city)','dish_liked']]\n",
        "  dish_liked['dish_liked'] = pd.DataFrame([a.strip() for a in dish_liked['dish_liked']])\n",
        "  return(dish_liked)\n",
        "\n",
        "\n",
        "# cuisines\n",
        "def atomizing_cuisines():\n",
        "  temp_cuisines = reviews[['name','listed_in(type)','listed_in(city)','cuisines']]\n",
        "  temp_cuisines['cuisines'] = temp_cuisines['cuisines'].str.split(',').tolist()\n",
        "  temp_cuisines['cuisines'] = temp_cuisines['cuisines'].fillna('0')\n",
        "\n",
        "  temp_cuisines['key'] = temp_cuisines['name'] + temp_cuisines['listed_in(type)'] + temp_cuisines['listed_in(city)']\n",
        "  temp_cuisines1 = temp_cuisines[['key','cuisines']]\n",
        "\n",
        "  cuisines = pd.DataFrame([[x] + [z] for x, y in temp_cuisines1.values for z in y],columns=temp_cuisines1.columns)\n",
        "\n",
        "  cuisines = cuisines.merge(temp_cuisines[['key','name','listed_in(type)','listed_in(city)']],how = 'left',on = 'key')\n",
        "  cuisines = cuisines[['key','name','listed_in(type)','listed_in(city)','cuisines']]\n",
        "  return(cuisines)\n",
        "\n",
        "# reviews_list\n",
        "def atomizing_reviews_list():\n",
        "  # temp_reviews_list = reviews[['name','listed_in(type)','listed_in(city)','reviews_list']]\n",
        "  # temp_reviews_list['reviews_list'] = temp_reviews_list['reviews_list'].str.split(',').tolist()\n",
        "  # temp_reviews_list['reviews_list'] = temp_reviews_list['reviews_list'].fillna('0')\n",
        "\n",
        "  # temp_reviews_list['key'] = temp_reviews_list['name'] + temp_reviews_list['listed_in(type)'] + temp_reviews_list['listed_in(city)']\n",
        "  # temp_reviews_list1 = temp_reviews_list[['key','reviews_list']]\n",
        "\n",
        "  # reviews_list = pd.DataFrame([[x] + [z] for x, y in temp_reviews_list1.values for z in y],columns=temp_reviews_list1.columns)\n",
        "\n",
        "  # reviews_list = reviews_list.merge(temp_reviews_list[['key','name','listed_in(type)','listed_in(city)']],how = 'left',on = 'key')\n",
        "  # reviews_list = reviews_list[['key','name','listed_in(type)','listed_in(city)','reviews_list']]\n",
        "  # return(reviews_list)\n",
        "  for j in range(0,len(reviews.reviews_list[0])):\n",
        "    temp_re_1 = str(reviews.reviews_list[j]).replace(\"\\\\\",\"\").replace(\"\\'\",\"\").replace(\"RATEDn\",\"\")\n",
        "    temp_re_2 = pd.DataFrame(re.split(\"\\), \\(\",temp_re_1))\n",
        "\n",
        "    cust_rate_1s = []\n",
        "    cust_review_1s = []\n",
        "    try_count = 0\n",
        "    except_count = 0\n",
        "    for i in range(0,len(temp_re_2[0])):\n",
        "      try:\n",
        "        cust_rate_1s.append(temp_re_2[0][i].split(\",\",1)[0])\n",
        "        cust_review_1s.append(temp_re_2[0][i].split(\",\",1)[1])\n",
        "\n",
        "        cust_review_dict = {'Cust_Rate': cust_rate_1s,\n",
        "                            'Review': cust_review_1s\n",
        "                            }\n",
        "\n",
        "        cust_review_df_1s = pd.DataFrame(cust_review_dict,columns = {\"Cust_Rate\",\"Review\"})\n",
        "        cust_review_df_1s['key'] = reviews.key[j]\n",
        "        try_count += 1\n",
        "      except:\n",
        "        except_count += 1\n",
        "\n",
        "        pass\n",
        "\n",
        "    if j == 0:\n",
        "      cust_review_final_df = cust_review_df_1s.copy()\n",
        "    else:\n",
        "      cust_review_final_df = pd.concat([cust_review_final_df,cust_review_df_1s],axis = 0)\n",
        "\n",
        "\n",
        "  # Remiving wrong entries\n",
        "  cust_review_final_df['Cust_Rate'] = cust_review_final_df['Cust_Rate'].replace(\"\\[\\(\",\"\",regex = True)\n",
        "  cust_review_final_df = cust_review_final_df[cust_review_final_df.Cust_Rate != \"veg biryani n Raita)\"]\n",
        "  cust_review_final_df = cust_review_final_df[cust_review_final_df.Cust_Rate != \"None\"]\n",
        "  cust_review_final_df['Cust_Rate'] = cust_review_final_df['Cust_Rate'].replace(\"Rated \",\"\",regex = True).astype('float')\n",
        "  cust_review_final_df['Review'] = cust_review_final_df['Review'].replace(\"\\)\\]\",\"\",regex = True)\n",
        "\n",
        "  return(cust_review_final_df)\n",
        "\n",
        "# menu_item\n",
        "def atomizing_menu_item():\n",
        "  temp_menu_item = reviews[['name','listed_in(type)','listed_in(city)','menu_item']]\n",
        "  temp_menu_item['menu_item'] = temp_menu_item['menu_item'].apply(lambda x: x.replace('[','').replace(']',''))\n",
        "  temp_menu_item['menu_item'] = temp_menu_item['menu_item'].apply(lambda x: x.replace(\"'\",\"\"))\n",
        "  temp_menu_item['menu_item'] = pd.DataFrame([a.strip() for a in temp_menu_item['menu_item']])\n",
        "  temp_menu_item['menu_item'] = temp_menu_item['menu_item'].str.split(',').tolist()\n",
        "  temp_menu_item['menu_item'] = temp_menu_item['menu_item'].fillna('0')\n",
        "\n",
        "  temp_menu_item['key'] = temp_menu_item['name'] + temp_menu_item['listed_in(type)'] + temp_menu_item['listed_in(city)']\n",
        "  temp_menu_item1 = temp_menu_item[['key','menu_item']]\n",
        "\n",
        "  menu_item = pd.DataFrame([[x] + [z] for x, y in temp_menu_item1.values for z in y],columns=temp_menu_item1.columns)\n",
        "\n",
        "  menu_item = menu_item.merge(temp_menu_item[['key','name','listed_in(type)','listed_in(city)']],how = 'left',on = 'key')\n",
        "  menu_item = menu_item[['key','name','listed_in(type)','listed_in(city)','menu_item']]\n",
        "  menu_item['menu_item'] = pd.DataFrame([a.strip() for a in menu_item['menu_item']])\n",
        "  return(menu_item)\n",
        "\n",
        "\n",
        "def convert_lower_case(data):\n",
        "    return np.char.lower(data)\n",
        "\n",
        "def remov_duplicates(input): \n",
        "  \n",
        "    # split input string separated by space \n",
        "    input = input.split(\" \") \n",
        "  \n",
        "    # joins two adjacent elements in iterable way \n",
        "    for i in range(0, len(input)): \n",
        "        input[i] = \"\".join(input[i]) \n",
        "  \n",
        "    # now create dictionary using counter method \n",
        "    # which will have strings as key and their  \n",
        "    # frequencies as value \n",
        "    UniqW = Counter(input) \n",
        "  \n",
        "    # joins two adjacent elements in iterable way \n",
        "    s = \" \".join(UniqW.keys()) \n",
        "    return(s)\n",
        "\n",
        "def remove_stop_words(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words and len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text\n",
        "\n",
        "def stemming(data):\n",
        "    stemmer= PorterStemmer()\n",
        "    \n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + stemmer.stem(w)\n",
        "    return new_text\n",
        "\n",
        "def convert_numbers(data):\n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            w = num2words(int(w))\n",
        "        except:\n",
        "            a = 0\n",
        "        new_text = new_text + \" \" + w\n",
        "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
        "    return new_text\n",
        "\n",
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")\n",
        "\n",
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        data = np.char.replace(data, symbols[i], ' ')\n",
        "        data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "    return data\n",
        "\n",
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data) #remove comma seperately\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_stop_words(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = convert_numbers(data)\n",
        "    data = stemming(data) #needed again as we need to stem the words\n",
        "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
        "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
        "    data = remov_duplicates(data)\n",
        "    return data\n",
        "\n",
        "def text_similarity(a,b):\n",
        "  x = (fuzz.ratio(a,b))\n",
        "  y = (fuzz.partial_ratio(a,b))\n",
        "  z = (fuzz.token_sort_ratio(a,b))\n",
        "  return(max(x,y,z))\n",
        "\n",
        "def recomend_restaurant(preference):\n",
        "  vec_bow = dictionary.doc2bow(preference.lower().split())\n",
        "\n",
        "  vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
        "  index = similarities.MatrixSimilarity(lsi[doc_term_matrix])\n",
        "  unsorted_similarity = index[vec_lsi]\n",
        "  sorted_similarity = sorted(enumerate(unsorted_similarity), key=lambda item: -item[1])\n",
        "  recommend_rest_list = []\n",
        "  i = 1\n",
        "  for index, similarity in sorted_similarity:\n",
        "      # print(similarity, final_review_df.key[index] ,cor[index])\n",
        "      recommend_rest_list.append([final_review_df.key[index],i])\n",
        "      i+=1\n",
        "  return(recommend_rest_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogs82Ogv261L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Data Normalization\n",
        "# Restaurant type\n",
        "rest_type = atomizing_rest_type()\n",
        "\n",
        "# Liked Dish\n",
        "dish_liked = atomizing_liked_dishes()\n",
        "\n",
        "# Cusines\n",
        "cuisines = atomizing_cuisines()\n",
        "\n",
        "# Reviews list\n",
        "# reviews_list = atomizing_reviews_list()\n",
        "cust_review_final_df = atomizing_reviews_list()\n",
        "\n",
        "# Menu item\n",
        "menu_item = atomizing_menu_item()\n",
        "\n",
        "# Base data\n",
        "reviews = reviews[['name','online_order','book_table','rate','votes','location','approx_cost(for two people)',\n",
        "                  'listed_in(type)','listed_in(city)']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjUjLXh9TKge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dish_all_rest = pickle.load(open(\"dish_all_rest1.pkl\",'rb'))\n",
        "cuisine_all_rest = pickle.load(open(\"cuisine_all_rest1.pkl\",'rb'))\n",
        "menu_all_rest = pickle.load(open(\"menu_all_rest1.pkl\",'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDlQBESzGmid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# menu_item.menu_item.unique()[1:15]\n",
        "# cuisines.cuisines\n",
        "# dish_liked.dish_liked\n",
        "dish_all_rest = []\n",
        "\n",
        "for j in dish_liked.key.unique():\n",
        "  temp_dish_1_rest = \"\"\n",
        "  for i in dish_liked[dish_liked.key == j][\"dish_liked\"]:\n",
        "    # temp_dish_1_rest.append(i)\n",
        "    temp_dish_1_rest = temp_dish_1_rest + i + \" \"\n",
        "  dish_all_rest.append([j,temp_dish_1_rest])\n",
        "  \n",
        "dish_all_rest = pd.DataFrame(dish_all_rest,columns = ['key','dish_liked'])\n",
        "\n",
        "\n",
        "# Cuisine\n",
        "cuisine_all_rest = []\n",
        "\n",
        "for j in cuisines.key.unique():\n",
        "  temp_cuisine_1_rest = \"\"\n",
        "  for i in cuisines[cuisines.key == j][\"cuisines\"]:\n",
        "    # temp_cuisine_1_rest.append(i)\n",
        "    temp_cuisine_1_rest = temp_cuisine_1_rest + i + \" \"\n",
        "\n",
        "  cuisine_all_rest.append([j,temp_cuisine_1_rest])\n",
        "\n",
        "cuisine_all_rest = pd.DataFrame(cuisine_all_rest,columns = ['key','cuisines'])\n",
        "\n",
        "\n",
        "# menu item\n",
        "menu_all_rest = []\n",
        "\n",
        "for j in menu_item.key.unique():\n",
        "  temp_menu_1_rest = \"\"\n",
        "  for i in menu_item[menu_item.key == j][\"menu_item\"]:\n",
        "    # temp_menu_1_rest.append(i)\n",
        "    temp_menu_1_rest = temp_menu_1_rest + i + \" \"\n",
        "\n",
        "  menu_all_rest.append([j,temp_menu_1_rest])\n",
        "\n",
        "menu_all_rest = pd.DataFrame(menu_all_rest,columns = ['key','menu_item'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b38xyVzKoo5R",
        "colab_type": "text"
      },
      "source": [
        "**Recommendation using NLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEfJ2OtrCmuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Based on cuisine \n",
        "score = []\n",
        "for i in cuisines.cuisines.unique():\n",
        "  score.append([text_similarity(i,cuisine),i])\n",
        "\n",
        "Matching_cuisine = pd.DataFrame(score,columns = [\"score\",\"cuisines\"]).sort_values(by = \"score\", ascending = False)\n",
        "Matching_cuisine = Matching_cuisine[Matching_cuisine.score > 50]\n",
        "\n",
        "# Req cuisine restaurant\n",
        "# cuisines\n",
        "req_cuisine_rest = cuisines.merge(Matching_cuisine, how = 'inner', on = 'cuisines')\n",
        "req_cuisine_rest = req_cuisine_rest[['key','cuisines']].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQHcNYxJAJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# req_cuisine_rest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBimuZuVDtV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Based on location\n",
        "score = []\n",
        "for i in cuisines['listed_in(city)'].unique():\n",
        "  score.append([text_similarity(i,location),i])\n",
        "\n",
        "Matching_location = pd.DataFrame(score,columns = ['score','location']).sort_values(by = 'score', ascending = False).head(1)[['location']]\n",
        "\n",
        "rest_location = cuisines[['key','listed_in(city)']].drop_duplicates()\n",
        "\n",
        "req_cuisine_loc_rest = req_cuisine_rest.merge(rest_location, how = 'left', on = 'key')\n",
        "req_cuisine_loc_rest = req_cuisine_loc_rest[req_cuisine_loc_rest['listed_in(city)'].isin(Matching_location['location'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTWLeKzj0dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reviews\n",
        "pickle.dump(reviews,open(\"./reviews.pkl\", 'wb'))\n",
        "!cp reviews.pkl drive/My\\ Drive/Test\n",
        "\n",
        "# pickle.dump(lsi, open(\"./lsi.pkl\", 'wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Rnwg9ADtaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# req_cuisine_loc_rest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeV99LH2X4ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Based on user preference\n",
        "# We consider review with rating 4 and above for customer preference matching\n",
        "Req_reviews = cust_review_final_df[cust_review_final_df.Cust_Rate >= 4.0]\n",
        "unique_stores = Req_reviews.key.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_xwZviHbx5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking all the key words in the review for all the restaurant\n",
        "final_reviews = {'rest_key':'rest_key_words'}\n",
        "\n",
        "for j in unique_stores:\n",
        "  temp_review_1 = \"\"\n",
        "  for i in set(Req_reviews[Req_reviews.key == j].Review.to_list()):\n",
        "    temp_review_1 += i\n",
        "  temp_review_1 = preprocess(temp_review_1)\n",
        "\n",
        "  final_reviews[str(j)] = temp_review_1\n",
        "\n",
        "final_review_df = [(k, v) for k, v in final_reviews.items()] \n",
        "final_review_df = pd.DataFrame(final_review_df,columns = ['key','rev'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBxZL-vweE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final_review_df\n",
        "\n",
        "temp_All_key_words = final_review_df.merge(dish_all_rest, how = 'outer', on = 'key')\n",
        "temp_All_key_words1 = temp_All_key_words.merge(cuisine_all_rest, how = 'outer', on = 'key')\n",
        "temp_All_key_words2 = temp_All_key_words1.merge(menu_all_rest, how = 'outer', on = 'key')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRQho366WZu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_All_key_words2[temp_All_key_words2.isna()] = \"\"\n",
        "temp_All_key_words2['new key words'] = temp_All_key_words2['rev'] + temp_All_key_words2['dish_liked'] + temp_All_key_words2['cuisines'] + temp_All_key_words2['menu_item']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8jylq03X8Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_review_df = temp_All_key_words2[['key','new key words']]\n",
        "final_review_df.rename(columns = {'new key words':'rev'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFTQ-RYUalYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(final_review_df, open(\"./final_review_df.pkl\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPROKWzhawUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp final_review_df.pkl drive/My\\ Drive/Test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz3drFqLFuwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modelling review in multi dimentianal space\n",
        "def preprocessing(): # this should not be in code i\n",
        "    for document in final_review_df.rev:\n",
        "        doc = strip_numeric(stem_text(document))\n",
        "        yield gensim.utils.tokenize(doc, lower=True)\n",
        "\n",
        "texts = preprocessing() # ii\n",
        "dictionary = corpora.Dictionary(texts) # iii # dictionaly to be pickled\n",
        "\n",
        "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in preprocessing()] \n",
        "tfidf = models.TfidfModel(doc_term_matrix)\n",
        "corpus_tfidf = tfidf[doc_term_matrix] # iv # to be pickeled\n",
        "\n",
        "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary)  # initialize an LSI transformation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhueWei2O-Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ranking restaurant based on customer preference\n",
        "# Finding distance and between documents and ranking\n",
        "# additional = \"pork\"\n",
        "Matching_rest_rank = recomend_restaurant(additional)\n",
        "req_pref_rest = pd.DataFrame(Matching_rest_rank,columns= [\"key\",\"rank\"]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGqVud4lS1B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Finding Top 3 restaurant to recommend\n",
        "top_3_rest = req_cuisine_loc_rest.merge(req_pref_rest, how = 'left', on = 'key').sort_values(by = 'rank', ascending = True).head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vgY6BBZK7Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['key'] = reviews['name'] + reviews['listed_in(type)'] + reviews['listed_in(city)']\n",
        "top_3_rest = top_3_rest[['key']].merge(reviews, how = 'left', on = 'key')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYeEbKUALdW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "75b98830-3f7b-4811-ab19-99f7699e9dc4"
      },
      "source": [
        "Rank = 1\n",
        "for i in top_3_rest.key:\n",
        "  # print(Rank,\": \",i)\n",
        "  print(Rank,\": \",top_3_rest[top_3_rest.key == i].name.to_list()[0])\n",
        "  print(\"\\t Cusines:\",cuisines[cuisines.key == i].cuisines.to_list())\n",
        "  print(\"\\t Location:\",top_3_rest[top_3_rest.key == i]['listed_in(city)'].to_list())\n",
        "\n",
        "  Rank += 1\n",
        "# cuisines[cuisines.key == i].cuisines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 :  Hotel Shangrila\n",
            "\t Cusines: ['Chinese', ' Momos']\n",
            "\t Location: ['MG Road']\n",
            "2 :  Harima\n",
            "\t Cusines: ['Japanese']\n",
            "\t Location: ['MG Road']\n",
            "3 :  Hae Kum Gang\n",
            "\t Cusines: ['Korean', ' Chinese', ' Japanese']\n",
            "\t Location: ['MG Road']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNdl7B-6ZBLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}